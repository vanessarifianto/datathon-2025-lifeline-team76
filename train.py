# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NvlH6pfS1cNq0so4Qpt0NxQ3FebfOsXk
"""

import os, json
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import balanced_accuracy_score, f1_score, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE

# ---- Configuration ----
MODELS_DIR  = Path("models");  MODELS_DIR.mkdir(parents=True, exist_ok=True)
OUTPUTS_DIR = Path("outputs"); OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)
EXCEL_PATH = None  # set to local "CTG.xls" if uploaded; else it will download from URL
URL = "https://archive.ics.uci.edu/ml/machine-learning-databases/00193/CTG.xls"
USE_SHAP = True  # set False to skip SHAP (faster)

FEATURES = [
    'LB','AC','FM','UC','ASTV','mSTV','ALTV','mLTV',
    'DL','DS','DP','Width','Min','Max','Nmax','Nzeros',
    'Mode','Mean','Median','Variance','Tendency'
]

# ---- Data loader (auto-detect label) ----
def load_ctg(excel_path=None, url=URL):
    src = excel_path if excel_path is not None else url
    xl = pd.ExcelFile(src)
    target_candidates = ["NSP", "CLASS", "CLASS\n"]
    for sheet in xl.sheet_names:
        df = pd.read_excel(src, sheet_name=sheet)
        df.columns = df.columns.astype(str).str.strip()
        for t in target_candidates:
            if t in df.columns:
                return df, t
    raise ValueError(f"Target not found in sheets: {xl.sheet_names}")

df, target_col = load_ctg(EXCEL_PATH)
df.columns = df.columns.astype(str).str.strip()
print("Using target:", target_col)

# ---- Select features/target ----
features = [c for c in FEATURES if c in df.columns]
data = df[features + [target_col]].dropna().copy()
y = data[target_col].astype(int)
if y.min() == 1:  # map 1,2,3 → 0,1,2
    y = y - 1
X = data[features].copy()
print("Class distribution:\n", y.value_counts())

# ---- Train/test split ----
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)

# ---- Pipeline + CV helpers ----
def make_pipe(model):
    return Pipeline(steps=[
        ("scaler", StandardScaler()),
        ("smote", SMOTE(random_state=42)),
        ("clf", model),
    ])

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# ---- Baselines (LogReg, RF 300) ----
baselines = {
    "LogReg": LogisticRegression(max_iter=1000, class_weight="balanced", random_state=42),
    "RF300":  RandomForestClassifier(n_estimators=300, class_weight="balanced", random_state=42),
}
for name, mdl in baselines.items():
    scores = cross_validate(
        make_pipe(mdl), X, y, cv=cv,
        scoring={"macro_f1":"f1_macro","bal_acc":"balanced_accuracy"},
        n_jobs=-1
    )
    print(f"{name} | Macro-F1={scores['test_macro_f1'].mean():.3f} | BalAcc={scores['test_bal_acc'].mean():.3f}")

# ---- GridSearchCV for RF ----
rf_pipe = make_pipe(RandomForestClassifier(class_weight="balanced", random_state=42))
param_grid = {
    "clf__n_estimators": [200, 300, 500],
    "clf__max_depth": [None, 6, 10],
    "clf__min_samples_split": [2, 5],
}
grid = GridSearchCV(rf_pipe, param_grid, cv=cv, scoring="f1_macro", n_jobs=-1, verbose=1)
grid.fit(X, y)
best_model = grid.best_estimator_
print("Best RF params:", grid.best_params_)
print("Best Macro-F1 (CV):", grid.best_score_)

# ---- Hold-out evaluation ----
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)
bal = balanced_accuracy_score(y_test, y_pred)
mf1 = f1_score(y_test, y_pred, average="macro")
print("\nHold-out evaluation (best RF):")
print("Balanced Acc:", bal)
print("Macro F1    :", mf1)
print(classification_report(y_test, y_pred))

# ---- Confusion matrix plot ----
cm = confusion_matrix(y_test, y_pred)
plt.figure()
sns.heatmap(cm, annot=True, fmt='d', cmap="Blues")
plt.title("RandomForest (Best) — Confusion Matrix")
plt.xlabel("Predicted"); plt.ylabel("True")
plt.tight_layout()
plt.savefig(OUTPUTS_DIR / "confusion_matrix.png", dpi=200)
plt.show()

# ---- Feature importances (CSV + plot) ----
rf_fitted = best_model.named_steps["clf"]
importances = pd.Series(rf_fitted.feature_importances_, index=features).sort_values(ascending=False)
importances.to_csv(OUTPUTS_DIR / "feature_importances.csv")
plt.figure(figsize=(6,5))
importances.sort_values().plot(kind="barh")
plt.title("Random Forest Feature Importances")
plt.xlabel("Importance")
plt.tight_layout()
plt.savefig(OUTPUTS_DIR / "feature_importances.png", dpi=200)
plt.show()

# ---- Save metrics + model weights ----
with open(OUTPUTS_DIR / "metrics.json", "w", encoding="utf-8") as f:
    json.dump({"balanced_accuracy": float(bal), "macro_f1": float(mf1)}, f, indent=2)

import joblib
weights_path = MODELS_DIR / "rf_smote_pipeline.joblib"
joblib.dump(best_model, weights_path)
print("Saved model to:", weights_path.resolve())
print("Artifacts in:", OUTPUTS_DIR.resolve())

# ---- Optional: SHAP summary ----
if USE_SHAP:
    try:
        import shap
        explainer = shap.TreeExplainer(rf_fitted)
        X_std = best_model.named_steps["scaler"].transform(X_test)
        shap_values = explainer.shap_values(X_std)
        shap.summary_plot(shap_values, X_std, feature_names=features)
    except Exception as e:
        print("SHAP skipped:", e)